{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Calibrated spike inference with Cascade (batch script).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HelmchenLabSoftware/Cascade/blob/master/Demo%20scripts/Calibrated_spike_inference_with_Cascade_(batch_script).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECkMdA5pMRSY"
      },
      "source": [
        "# CASCADE\n",
        "\n",
        "## Calibrated spike inference from calcium imaging data using deep networks (batch script)\n",
        "Written and maintained by [Peter Rupprecht](https://github.com/PTRRupprecht) from the [Helmchen Lab](https://www.hifo.uzh.ch/en/research/helmchen.html).\n",
        "The project started as a collaboration of the Helmchen Lab and the [Friedrich Lab](https://www.fmi.ch/research-groups/groupleader.html?group=119). Feedback goes to [Peter Rupprecht](mailto:p.t.r.rupprecht+cascade@gmail.com).\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This colaboratory notebook runs on servers in the cloud. It uses an algorithm based on deep networks for spike inference (CASCADE, described in this **[Resource Article](https://www.nature.com/articles/s41593-021-00895-5)** published in Nature Neuroscience). Here, you can test the algorithm and use it without any installation on your computer. You just have to sequentially **press the play buttons (\"Run cell\")** on the left of each box, and the code will be executed.\n",
        "\n",
        "* If you want to **see the algorithm in action**, just execute the cells without any modifications. Enjoy!\n",
        "\n",
        "* If you want to **upload your own data**, make predictions and download the saved files, you have to modify the variable names and follow the instructions. Usually no or very little modifications of the code is required.\n",
        "\n",
        "* If you want to integrate CASCADE into **your local data analysis pipeline**, we suggest you take a look at the [Github repository](https://github.com/HelmchenLabSoftware/Calibrated-inference-of-spiking)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOWWhpFGk7lA"
      },
      "source": [
        "##1. Download repository into the Colab Notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "yfupSpg7FtpN"
      },
      "source": [
        "#@markdown The Github repository with all custom functions, the ground truth datasets and the pretrained models is copied to the environment of this notebook. This will take a couple of seconds.\n",
        "\n",
        "#@markdown *Note: You can check the code underlying each cell by double-clicking on it.*\n",
        "\n",
        "import os\n",
        "\n",
        "# If in Colab and not yet downloaded, download GitHub repository and change working directory\n",
        "if os.getcwd() == '/content':\n",
        "    !git clone https://github.com/HelmchenLabSoftware/Cascade\n",
        "    os.chdir('Cascade')\n",
        "\n",
        "# If executed as jupyter notebook on own computer, change to parent directory for imports\n",
        "if os.path.basename( os.getcwd() ) == 'Demo scripts':\n",
        "    %cd ..\n",
        "    print('New working directory:', os.getcwd() )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBMThw3clDu4"
      },
      "source": [
        "##2. Import required python packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "u-cs8Jze8MpX"
      },
      "source": [
        "#@markdown Downloads packages from public repository, and packages from Cascade.\n",
        "\n",
        "%%capture\n",
        "!pip install ruamel.yaml\n",
        "\n",
        "# standard python packages\n",
        "import os, warnings\n",
        "import glob\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import matplotlib.pyplot as plt\n",
        "import ruamel.yaml as yaml\n",
        "yaml = yaml.YAML(typ='rt')\n",
        "\n",
        "# cascade2p packages, imported from the downloaded Github repository\n",
        "from cascade2p import checks\n",
        "checks.check_packages()\n",
        "from cascade2p import cascade # local folder\n",
        "from cascade2p.utils import plot_dFF_traces, plot_noise_level_distribution, plot_noise_matched_ground_truth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19xLRfZXlG3z"
      },
      "source": [
        "##3. Define the function to load ΔF/F traces\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "slzhhQLd8YeP"
      },
      "source": [
        "#@markdown ΔF/F traces must be saved as \\*.npy-files (for Python) or \\*.mat-files (for Matlab/Python) as a single large matrix named **`dF_traces`** (neurons x time). ΔF/F values of the input should be numeric, not in percent (e.g. 0.5 instead of 50%). For different input formats, the code in this box can be modified (it\\'s not difficult).\n",
        "\n",
        "def load_neurons_x_time(file_path):\n",
        "    \"\"\"Custom method to load data as 2d array with shape (neurons, nr_timepoints)\"\"\"\n",
        "\n",
        "    if file_path.endswith('.mat'):\n",
        "      traces = sio.loadmat(file_path)['dF_traces']\n",
        "      # PLEASE NOTE: If you use mat73 to load large *.mat-file, be aware of potential numerical errors, see issue #67 (https://github.com/HelmchenLabSoftware/Cascade/issues/67)\n",
        "\n",
        "    elif file_path.endswith('.npy'):\n",
        "      traces = np.load(file_path, allow_pickle=True)\n",
        "      # if saved data was a dictionary packed into a numpy array (MATLAB style): unpack\n",
        "      if traces.shape == ():\n",
        "        traces = traces.item()['dF_traces']\n",
        "\n",
        "    else:\n",
        "      raise Exception('This function only supports .mat or .npy files.')\n",
        "\n",
        "    print('Traces standard deviation:', np.nanmean(np.nanstd(traces,axis=1)))\n",
        "    if np.nanmedian(np.nanstd(traces,axis=1)) > 2:\n",
        "      print('Fluctuations in dF/F are very large, probably dF/F is given in percent. Traces are divided by 100.')\n",
        "      return traces/100\n",
        "    else:\n",
        "        return traces\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9yjbje7lQIA"
      },
      "source": [
        "##4. Batch process files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "wKyE8JVV8fMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "775a2a93-623a-4110-821d-d3ec74053fe7"
      },
      "source": [
        "#@markdown If you are testing the script, you can leave everything unchanged. If you want to apply the algorithm to your own data, you have to upload your data first. The paragraph above tells you how to format and name the files. You can do this by clicking on the **folder symbol (\"Files\")** on the left side of the Colaboratory notebook. Next, indicate the path of the uploaded files in the variables **example_folder** and the pattern **`file_pattern`**. The file pattern with the asterisk (\\*) as placeholder indicates the general pattern of all files. All files for this pattern will be processed. Finally, indicate the sampling rate of your recordings in the variable **`frame_rate`**.\n",
        "\n",
        "example_folder = \"Example_datasets/Allen-Brain-Observatory-Visual-Coding-30Hz/\" #@param {type:\"string\"}\n",
        "\n",
        "file_pattern = \"Experiment_55*.mat\" #@param {type:\"string\"}\n",
        "\n",
        "frame_rate = 30 #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "#@markdown Select and download the model that fits to your dataset (frame rate, training data; see FAQ for more details) and assign to variable **`model_name`**.\n",
        "model_name = \"Global_EXC_30Hz_smoothing25ms_causalkernel\" #@param {type:\"string\"}\n",
        "cascade.download_model( model_name,verbose = 1)\n",
        "\n",
        "\n",
        "all_file_names = glob.glob(example_folder+file_pattern)\n",
        "\n",
        "for file_index,example_file in enumerate(all_file_names):\n",
        "\n",
        "  try:\n",
        "\n",
        "    traces = load_neurons_x_time( example_file )\n",
        "    print('Number of neurons in dataset:', traces.shape[0])\n",
        "    print('Number of timepoints in dataset:', traces.shape[1])\n",
        "\n",
        "  except Exception as e:\n",
        "\n",
        "    print('\\nSomething went wrong!\\nEither the target file is missing, in this case please provide the correct location.\\nOr your file is not yet completely uploaded, in this case wait until the upload is completed.\\n')\n",
        "\n",
        "    print('Error message: '+str(e))\n",
        "\n",
        "  #@markdown If this takes too long, make sure that the GPU runtime is activated (*Menu > Runtime > Change Runtime Type*).\n",
        "\n",
        "  total_array_size = traces.itemsize*traces.size*64/1e9\n",
        "\n",
        "  # If the expected array size is too large for the Colab Notebook, split up for processing\n",
        "  if total_array_size < 10:\n",
        "\n",
        "    spike_prob = cascade.predict( model_name, traces, verbosity=1 )\n",
        "\n",
        "  # Will only be use for large input arrays (long recordings or many neurons)\n",
        "  else:\n",
        "\n",
        "    print(\"Split analysis into chunks in order to fit into Colab memory.\")\n",
        "\n",
        "    # pre-allocate array for results\n",
        "    spike_prob = np.zeros((traces.shape))\n",
        "    # nb of neurons and nb of chuncks\n",
        "    nb_neurons = traces.shape[0]\n",
        "    nb_chunks = int(np.ceil(total_array_size/10))\n",
        "\n",
        "    chunks = np.array_split(range(nb_neurons), nb_chunks)\n",
        "    # infer spike rates independently for each chunk\n",
        "    for part_array in range(nb_chunks):\n",
        "      spike_prob[chunks[part_array],:] = cascade.predict( model_name, traces[chunks[part_array],:] )\n",
        "\n",
        "\n",
        "  #@markdown By default saves as variable **`spike_prob`** both to a *.mat-file and a *.npy-file. You can uncomment the file format that you do not need or leave it as it is.\n",
        "\n",
        "  folder = os.path.dirname(example_folder)\n",
        "  file_name = 'predictions_' + os.path.splitext( os.path.basename(example_file))[0]\n",
        "  save_path = os.path.join(folder, file_name)\n",
        "\n",
        "  # save as mat file\n",
        "  sio.savemat(save_path+'.mat', {'spike_prob':spike_prob})\n",
        "\n",
        "  # save as numpy file\n",
        "  np.save(save_path, spike_prob)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and extracting new model \"Global_EXC_30Hz_smoothing25ms_causalkernel\"...\n",
            "Pretrained model was saved in folder \"/content/Cascade/Pretrained_models/Global_EXC_30Hz_smoothing25ms_causalkernel\"\n",
            "Traces standard deviation: 0.079336\n",
            "Number of neurons in dataset: 74\n",
            "Number of timepoints in dataset: 6001\n",
            "\n",
            " \n",
            "The selected model was trained on 18 datasets, with 5 ensembles for each noise level, at a sampling rate of 30Hz, with a resampled ground truth that was smoothed with a causal kernel of a standard deviation of 25 milliseconds. \n",
            " \n",
            "\n",
            "Loaded model was trained at frame rate 30 Hz\n",
            "Given argument traces contains 74 neurons and 6001 frames.\n",
            "Noise levels (mean, std; in standard units): 0.93, 0.17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions for noise level 2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t... ensemble 0\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\t... ensemble 1\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "\t... ensemble 2\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\t... ensemble 3\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\t... ensemble 4\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\n",
            "Predictions for noise level 3:\n",
            "\tNo neurons for this noise level\n",
            "\n",
            "Predictions for noise level 4:\n",
            "\tNo neurons for this noise level\n",
            "\n",
            "Predictions for noise level 5:\n",
            "\tNo neurons for this noise level\n",
            "\n",
            "Predictions for noise level 6:\n",
            "\tNo neurons for this noise level\n",
            "\n",
            "Predictions for noise level 7:\n",
            "\tNo neurons for this noise level\n",
            "\n",
            "Predictions for noise level 8:\n",
            "\tNo neurons for this noise level\n",
            "\n",
            "Predictions for noise level 9:\n",
            "\tNo neurons for this noise level\n",
            "Spike rate inference done.\n",
            "Traces standard deviation: 0.079336\n",
            "Number of neurons in dataset: 74\n",
            "Number of timepoints in dataset: 6001\n",
            "\n",
            " \n",
            "The selected model was trained on 18 datasets, with 5 ensembles for each noise level, at a sampling rate of 30Hz, with a resampled ground truth that was smoothed with a causal kernel of a standard deviation of 25 milliseconds. \n",
            " \n",
            "\n",
            "Loaded model was trained at frame rate 30 Hz\n",
            "Given argument traces contains 74 neurons and 6001 frames.\n",
            "Noise levels (mean, std; in standard units): 0.93, 0.17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions for noise level 2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t... ensemble 0\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\t... ensemble 1\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\t... ensemble 2\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\t... ensemble 3\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "\t... ensemble 4\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\n",
            "Predictions for noise level 3:\n",
            "\tNo neurons for this noise level\n",
            "\n",
            "Predictions for noise level 4:\n",
            "\tNo neurons for this noise level\n",
            "\n",
            "Predictions for noise level 5:\n",
            "\tNo neurons for this noise level\n",
            "\n",
            "Predictions for noise level 6:\n",
            "\tNo neurons for this noise level\n",
            "\n",
            "Predictions for noise level 7:\n",
            "\tNo neurons for this noise level\n",
            "\n",
            "Predictions for noise level 8:\n",
            "\tNo neurons for this noise level\n",
            "\n",
            "Predictions for noise level 9:\n",
            "\tNo neurons for this noise level\n",
            "Spike rate inference done.\n"
          ]
        }
      ]
    }
  ]
}